import os
import time
import json
import torch
import gc
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë”© â”€â”€â”€
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# â”€â”€â”€ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ â”€â”€â”€
MODELS = [
    "kakaocorp/kanana-nano-2.1b-base",
]

# â”€â”€â”€ ìƒ˜í”Œ ì…ë ¥ â”€â”€â”€
SAMPLES = [
    {
        "stain": {"class": "blood", "advice": "ì°¬ë¬¼ì— ë¶ˆë¦° í›„ ë‹¨ë°±ì§ˆ ë¶„í•´ íš¨ì†Œê°€ í¬í•¨ëœ ì„¸ì œë¡œ ì„¸íƒí•˜ì„¸ìš”."},
        "labels": [
            "40ë„ ì´í•˜ì—ì„œ ì¼ë°˜ ì„¸íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "í‘œë°± ê¸ˆì§€: ì‚°ì†Œê³„, ì—¼ì†Œê³„ í‘œë°±ì œë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
            "ê·¸ëŠ˜ì—ì„œ ì˜·ê±¸ì´ë‚˜ ì¤„ì— ë„ì–´ ë§ë¦¬ì„¸ìš”."
        ]
    },
    {
        "stain": {"class": "coffee", "advice": "ì¦‰ì‹œ ì°¬ë¬¼ë¡œ í—¹êµ° í›„ ì‚°ì†Œê³„ í‘œë°±ì œë¥¼ ì‚¬ìš©í•´ ì„¸íƒí•˜ì„¸ìš”."},
        "labels": [
            "30ë„ì—ì„œ ì„¬ì„¸ ì„¸íƒí•˜ì„¸ìš”.",
            "ê±´ì¡°ê¸° ì‚¬ìš© ê¸ˆì§€",
            "ì¤‘ê°„ ì˜¨ë„ì—ì„œ ë‹¤ë¦¼ì§ˆí•˜ì„¸ìš”."
        ]
    }
]

# â”€â”€â”€ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â”€â”€â”€
def build_prompt(sample):
    return (
        f"ë‹¤ìŒì€ ì˜·ì˜ ì–¼ë£©ê³¼ ì„¸íƒ ê¸°í˜¸ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.\n"
        f"- ì–¼ë£© ì¢…ë¥˜: {sample['stain']['class']}\n"
        f"- ì¶”ì²œ ì„¸íƒë²•: {sample['stain']['advice']}\n"
        f"- ì„¸íƒ ê¸°í˜¸ í•´ì„: {', '.join(sample['labels'])}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì„¸íƒ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\në‹µë³€:"
    )

# â”€â”€â”€ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ â”€â”€â”€
def benchmark_model(model_name, samples):
    print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN, padding_side="left")
        tokenizer.pad_token = tokenizer.eos_token

        # dtype ìš°ì„ ìˆœìœ„: float16 â†’ float32 (bfloat16ì€ ìƒëµ)
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                trust_remote_code=True
            ).to("cuda")
        except (RuntimeError, ValueError) as e:
            print("âš ï¸ float16 ì˜¤ë¥˜ â†’ float32ë¡œ fallback")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                trust_remote_code=True
            ).to("cuda")

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {"model": model_name, "error": str(e), "results": []}

    results = []
    for i, sample in enumerate(samples):
        try:
            prompt = build_prompt(sample)
            input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

            start = time.time()
            output = model.generate(
                input_ids,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
            elapsed = time.time() - start

            decoded = tokenizer.decode(output[0], skip_special_tokens=True).split("ë‹µë³€:")[-1].strip()

            results.append({
                "input": sample,
                "elapsed_time_sec": round(elapsed, 2),
                "guide": decoded
            })

        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            results.append({
                "input": sample,
                "error": str(e)
            })

    del model, tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    return {"model": model_name, "results": results}

# â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€
def run_benchmark():
    for model in MODELS:
        print(f"\nâ³ ëª¨ë¸ ì‹¤í–‰ ì‹œì‘: {model}")
        try:
            result = benchmark_model(model, SAMPLES)
            save_path = f"benchmark_result_fixed_{model.replace('/', '_')}.json"
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            print(f"âŒ {model} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

# â”€â”€â”€ ì‹œì‘ì  â”€â”€â”€
if __name__ == "__main__":
    run_benchmark()
