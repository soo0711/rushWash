import os
import time
import json
import torch
import gc
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM

# â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ ë¡œë”© â”€â”€â”€
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# â”€â”€â”€ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ â”€â”€â”€
MODELS = [
    "kakaocorp/kanana-1.5-2.1b-base",
    "kakaocorp/kanana-nano-2.1b-base",
]

# â”€â”€â”€ ìƒ˜í”Œ ì…ë ¥ â”€â”€â”€
SAMPLES = [
    {
        "stain": {
            "class": "blood",
            "advice": "ì°¬ë¬¼ì— ë¶ˆë¦° í›„ ë‹¨ë°±ì§ˆ ë¶„í•´ íš¨ì†Œê°€ í¬í•¨ëœ ì„¸ì œë¡œ ì„¸íƒí•˜ì„¸ìš”.",
        },
        "labels": [
            "40ë„ ì´í•˜ì—ì„œ ì¼ë°˜ ì„¸íƒ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "í‘œë°± ê¸ˆì§€: ì‚°ì†Œê³„, ì—¼ì†Œê³„ í‘œë°±ì œë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
            "ê·¸ëŠ˜ì—ì„œ ì˜·ê±¸ì´ë‚˜ ì¤„ì— ë„ì–´ ë§ë¦¬ì„¸ìš”.",
        ],
    },
    {
        "stain": {
            "class": "coffee",
            "advice": "ì¦‰ì‹œ ì°¬ë¬¼ë¡œ í—¹êµ° í›„ ì‚°ì†Œê³„ í‘œë°±ì œë¥¼ ì‚¬ìš©í•´ ì„¸íƒí•˜ì„¸ìš”.",
        },
        "labels": [
            "30ë„ì—ì„œ ì„¬ì„¸ ì„¸íƒí•˜ì„¸ìš”.",
            "ê±´ì¡°ê¸° ì‚¬ìš© ê¸ˆì§€",
            "ì¤‘ê°„ ì˜¨ë„ì—ì„œ ë‹¤ë¦¼ì§ˆí•˜ì„¸ìš”.",
        ],
    },
]
CONTRADICTORY_SAMPLES = [
    {
        "stain": {
            "class": "blood",
            "advice": "ì•”ëª¨ë‹ˆì•„ ìš©ì•¡(ë¬¼:ì•”ëª¨ë‹ˆì•„ = 2:1)ì„ ë¬»í˜€ ì²˜ë¦¬í•œ ë’¤ ì„¸íƒí•˜ì„¸ìš”.",
        },
        "labels": [
            "ì„¸íƒ ê¸ˆì§€: ë¬¼ì„¸íƒì„ í•˜ì§€ ë§ˆì„¸ìš”.",
            "í‘œë°± ê¸ˆì§€: ì‚°ì†Œê³„, ì—¼ì†Œê³„ í‘œë°±ì œë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
            "ê±´ì¡° ê¸ˆì§€: ì—´ ê±´ì¡°ë‚˜ ìì—° ê±´ì¡° ëª¨ë‘ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.",
        ],
    },
    {
        "stain": {
            "class": "coffee",
            "advice": "ë¯¸ì§€ê·¼í•œ ë¬¼ì— ë¶ˆë¦° í›„ ì¤‘ì„±ì„¸ì œ ë˜ëŠ” ì‚°ì†Œê³„ í‘œë°±ì œë¡œ ì„¸íƒí•˜ì„¸ìš”.",
        },
        "labels": [
            "ë“œë¼ì´í´ë¦¬ë‹ ê¸ˆì§€: ì „ë¬¸ê°€ ì„¸íƒì†Œì— ë§¡ê¸°ëŠ” ê²ƒë„ ê¸ˆì§€ì…ë‹ˆë‹¤.",
            "ê±´ì¡°ê¸° ì‚¬ìš© ê¸ˆì§€",
            "ê³ ì˜¨ ë‹¤ë¦¼ì§ˆ ê°€ëŠ¥: ì•½ 200Â°C ì´í•˜ì—ì„œ ì‚¬ìš©.",
        ],
    },
    {
        "stain": {
            "class": "lipstick",
            "advice": "ì¤‘ì„±ì„¸ì œì™€ ë² ì´í‚¹ ì†Œë‹¤ë¥¼ ì„ì–´ ì–¼ë£©ì— ë°”ë¥´ê³  ë¬¸ì§€ë¥¸ ë’¤ ì„¸íƒí•˜ì„¸ìš”.",
        },
        "labels": [
            "ìŠµì‹ ì„¸íƒ ê¸ˆì§€: ì „ë¬¸ê°€ìš© ìŠµì‹ ì„¸íƒë„ ê¸ˆì§€ì…ë‹ˆë‹¤.",
            "í‘œë°± ê¸ˆì§€: ì‚°ì†Œê³„, ì—¼ì†Œê³„ í‘œë°±ì œë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.",
            "ë‹¤ë¦¼ì§ˆ ê¸ˆì§€: ì—´ì— ì˜í•´ ì†ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        ],
    },
]


# â”€â”€â”€ í”„ë¡¬í”„íŠ¸ êµ¬ì„± â”€â”€â”€
def build_prompt(sample):
    return (
        f"ì•„ë˜ëŠ” ì˜·ì— ë¬»ì€ ì–¼ë£©ê³¼ í•´ë‹¹ ì„¸íƒ ê¸°í˜¸ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n"
        f"- ì–¼ë£© ì¢…ë¥˜ëŠ” {sample['stain']['class']}ì´ë©°,\n"
        f"- í•´ë‹¹ ì–¼ë£©ì— ëŒ€í•´ ì¶”ì²œë˜ëŠ” ì„¸íƒë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {sample['stain']['advice']}\n"
        f"- ì„¸íƒ ê¸°í˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {', '.join(sample['labels'])}\n\n"
        "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì˜·ì„ ì–´ë–»ê²Œ ì„¸íƒí•´ì•¼ í•˜ëŠ”ì§€ ë¶€ë“œëŸ½ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”. "
        "ì‚¬ìš©ìì—ê²Œ ë§í•˜ë“¯ ì“°ë˜, ì¸ì‚¬ë§ ì—†ì´ ì§ì ‘ì ì¸ ëª…ë ¹í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\nì„¸íƒ ë°©ë²•:"
    )


# â”€â”€â”€ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ â”€â”€â”€
def benchmark_model(model_name, samples):
    print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, token=HF_TOKEN, padding_side="left"
        )
        tokenizer.pad_token = tokenizer.eos_token

        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                token=HF_TOKEN,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
            ).to("cuda")
        except (RuntimeError, ValueError) as e:
            if "addmm_impl_cpu_" in str(e) or "not implemented for" in str(e):
                print("âš ï¸ bfloat16 ë¬¸ì œ â†’ float32ë¡œ ì¬ì‹œë„")
                model = AutoModelForCausalLM.from_pretrained(
                    model_name, torch_dtype=torch.float32, trust_remote_code=True
                ).to("cuda")
            else:
                raise e

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return {"model": model_name, "error": str(e), "results": []}

    results = []
    for i, sample in enumerate(samples):
        try:
            prompt = build_prompt(sample)
            input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

            start = time.time()
            output = model.generate(
                input_ids,
                max_new_tokens=256,  # ëŠ˜ë¦¼
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
            )
            elapsed = time.time() - start

            decoded = (
                tokenizer.decode(output[0], skip_special_tokens=True)
                .split("ì„¸íƒ ë°©ë²•:")[-1]
                .strip()
            )

            results.append(
                {
                    "input": sample,
                    "elapsed_time_sec": round(elapsed, 2),
                    "guide": decoded,
                }
            )

        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            results.append({"input": sample, "error": str(e)})

    del model, tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    return {"model": model_name, "results": results}


# â”€â”€â”€ ì‹¤í–‰ â”€â”€â”€
def run_benchmark():
    for model in MODELS:
        print(f"\nâ³ ëª¨ë¸ ì‹¤í–‰ ì‹œì‘: {model}")
        try:
            result = benchmark_model(model, CONTRADICTORY_SAMPLES)
            save_path = f"benchmark_result_fixed_{model.replace('/', '_')}.json"
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            print(f"âŒ {model} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


# â”€â”€â”€ ì‹œì‘ì  â”€â”€â”€
if __name__ == "__main__":
    run_benchmark()
